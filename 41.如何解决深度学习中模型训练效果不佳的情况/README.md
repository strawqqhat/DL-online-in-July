```选择合适的损失函数```

神经网络的损失函数时非凸的，有多个局部最低点，目标是找到一个可用的低点。非凸函数时凹凸不平的，但是不同的损失函数凹凸程度不同，比如平方损失和交叉熵损失相比后者起伏更大，且后者更容易找到一个可用的最低点，从而达到优化的目的。

```选择合适的批量尺寸```

采用合适的批量尺寸进行学习，一方面可以减少计算量，一方卖弄有助于跳出局部最优点，因此要选择合适的批量尺寸。另一方面，批量尺寸取太大会陷入最不最小值，取太小会抖动严重。

```选择合适的激活函数```

使用激活函数把卷积层输出做非线性映射，但是要选择合适的激活函数。比如：sigmoid函数是一个平滑函数，且具有连续性和可微性，最大优点是非线性。该函数两端很缓，易发生学不动的情况产生梯度弥散；ReLU函数是现阶段设计神经网络时使用最广泛的激活函数，该函数为非线性映射，且简单能够缓解梯度弥散。

```选择合适的自适应学习率```

学习率过大会抖动厉害，导致没有优化提升；
学习率过小会导致下降太慢，训练缓慢；

```使用动量```

在梯度的基础上使用动量，有助于冲出局部最低点。