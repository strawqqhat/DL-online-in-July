1. 对于神经网络来说，网络的每一层相当于f(wx+b)，对于线性函数，其实相当于f(x)=x，那么在线性激活函数下，每一层相当于用一个矩阵去乘以x，那么多层就是反复用矩阵去乘以输入。根据矩阵的乘法法则，多个矩阵相乘得到一个大矩阵。所以线性激活函数下，多层网络与一层网络相当。比如两层的网络f(w1*f(w2x))=w1w2x=Wx。
2. 非线性变换是深度学习有效的原因之一。原因在于非线性相当于对空间进行变换，变换完成后相当于对问题空间进行简化，原来线性不可解的问题现在变得可以解了。

**注释**：如果不用激励函数，在这种情况下每一层输出都是上层输入的线性函数，这样无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机了。

