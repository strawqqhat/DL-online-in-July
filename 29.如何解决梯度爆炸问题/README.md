<font color='red'>重新设计网络模型</font>

* 在深度神经网络中，梯度爆炸可以通过重新设计更少的网络层数来解决
* 使用更小的批尺寸对网络训练也有好处
* 在循环神经网络中，训练过程中在更少的先前时间步上进行更新可以缓解梯度爆炸问题。

 <font color='red'>使用ReLU激活函数</font>
* 使用ReLU激活函数可以减少梯度爆炸，它也是最适合隐藏层的新实践。

 <font color='red'>使用长短期记忆网络</font>
 * 采用长短期记忆单元和相关的门类型神经元结构可以减少梯度爆炸问题。

 <font color='red'>使用梯度截断</font>
 * 在非常深且批尺寸较大的多层感知机网络和输入序列较长的LSTM中，仍有可能出现梯度爆炸。如果梯度爆炸仍然出现可以在训练过程中检查和限制梯度的大小。

 <font color='red'>使用权重正则化</font>
* 如果梯度爆炸仍然存在，可以尝试另一种办法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用L1惩罚项(权重绝对值)或L2惩罚项(权重平方).